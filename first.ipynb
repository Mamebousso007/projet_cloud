{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage, bigquery\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations GCP\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bq_client = bigquery.Client()\n",
    "\n",
    "PROJECT_ID = \"isi-group-m2-dsia\"\n",
    "BUCKET_NAME = \"m2dsia-sylla-mame-diarra-data\"\n",
    "DATASET_ID = \"dataset_mame_diarra_sylla\"\n",
    "TABLE_ID = f\"{bq_client.project}.{DATASET_ID}.transactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/transactions.csv\n"
     ]
    }
   ],
   "source": [
    "blobs = bucket.list_blobs(prefix=\"input/\")\n",
    "for blob in blobs:\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_folder(folder):\n",
    "    \"\"\"Liste les fichiers dans un dossier GCS.\"\"\"\n",
    "    return [blob.name for blob in bucket.list_blobs(prefix=folder) if not blob.name.endswith(\"/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(gcs_path, local_path):\n",
    "    \"\"\"Télécharge un fichier de GCS vers le local.\"\"\"\n",
    "    blob = bucket.blob(gcs_path)\n",
    "    blob.download_to_filename(local_path)\n",
    "    logging.info(f\"Téléchargé : {gcs_path} -> {local_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour uploader un fichier local vers GCS\n",
    "def upload_file(local_path, gcs_path):\n",
    "    \"\"\"Upload un fichier local vers GCS.\"\"\"\n",
    "    blob = bucket.blob(gcs_path)\n",
    "    blob.upload_from_filename(local_path)\n",
    "    logging.info(f\"Uploadé : {local_path} -> {gcs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded files/transactions.csv to input/transactions.csv\n"
     ]
    }
   ],
   "source": [
    "upload_blob('files/transactions.csv','input/transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3\n",
    "def copy_from_blob(blob, folder):\n",
    "    bucket.copy_blob(blob, bucket, folder+'/'+blob.name.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_from_file(file_name, folder):\n",
    "    blob = bucket.blob(file_name)\n",
    "    bucket.copy_blob(blob, bucket, folder+'/'+file_name.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour valider et nettoyer les données\n",
    "def validate_and_clean_data(local_file):\n",
    "    \"\"\"Valide et nettoie les données.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(local_file)\n",
    "\n",
    "        # Validation des colonnes requises\n",
    "        required_columns = [\n",
    "            \"transaction_id\", \"product_name\", \"category\", \"price\", \"quantity\", \"date\",\n",
    "            \"customer_name\", \"customer_email\"\n",
    "        ]\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(\"Colonnes manquantes dans le fichier d'entrée.\")\n",
    "\n",
    "        # Remplir les valeurs manquantes dans les colonnes obligatoires\n",
    "        df[\"product_name\"] = df[\"product_name\"].fillna(\"Produit inconnu\")\n",
    "        df[\"category\"] = df[\"category\"].fillna(\"Catégorie inconnue\")\n",
    "\n",
    "        # Conversion des types de données\n",
    "        df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "        df[\"quantity\"] = pd.to_numeric(df[\"quantity\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "        df[\"transaction_id\"] = pd.to_numeric(df[\"transaction_id\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "        # Filtrer les lignes avec des dates invalides\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "        valid_data = df.dropna(subset=[\"price\", \"quantity\", \"date\"])\n",
    "        invalid_data = df[~df.index.isin(valid_data.index)]\n",
    "\n",
    "        # Enregistrer les fichiers nettoyés et rejetés\n",
    "        cleaned_file = local_file.replace(\".csv\", \"_cleaned.csv\")\n",
    "        error_file = local_file.replace(\".csv\", \"_errors.csv\")\n",
    "        valid_data.to_csv(cleaned_file, index=False)\n",
    "        invalid_data.to_csv(error_file, index=False)\n",
    "\n",
    "        logging.info(f\"Fichier nettoyé : {cleaned_file}\")\n",
    "        logging.info(f\"Fichier d'erreurs : {error_file}\")\n",
    "        return cleaned_file, error_file\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur de validation/cleaning : {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger les données dans BigQuery\n",
    "def load_to_bigquery(file_path):\n",
    "    \"\"\"Charge un fichier dans BigQuery.\"\"\"\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"transaction_id\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "            bigquery.SchemaField(\"product_name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"category\", \"STRING\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"price\", \"FLOAT\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"quantity\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"date\", \"DATE\", mode=\"REQUIRED\"),\n",
    "            bigquery.SchemaField(\"customer_name\", \"STRING\", mode=\"NULLABLE\"),\n",
    "            bigquery.SchemaField(\"customer_email\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        ],\n",
    "        write_disposition=\"WRITE_APPEND\"  # Ajout des données sans écraser\n",
    "    )\n",
    "    uri = f\"gs://{BUCKET_NAME}/{file_path}\"\n",
    "    load_job = bq_client.load_table_from_uri(uri, TABLE_ID, job_config=job_config)\n",
    "    load_job.result()  # Attendre la fin du job\n",
    "    logging.info(f\"Données chargées dans BigQuery depuis : {uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour déplacer un fichier vers le dossier done/\n",
    "def move_to_done(file_path):\n",
    "    \"\"\"Déplace un fichier vers le dossier done/.\"\"\"\n",
    "    source_blob = bucket.blob(file_path)\n",
    "    new_blob = bucket.blob(file_path.replace(\"clean\", \"done\"))\n",
    "    bucket.copy_blob(source_blob, bucket, new_blob.name)\n",
    "    source_blob.delete()\n",
    "    logging.info(f\"Fichier déplacé : {file_path} -> {new_blob.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "def process_files():\n",
    "    \"\"\"Traite les fichiers du dossier input/.\"\"\"\n",
    "    files = list_files_in_folder(\"input/\")\n",
    "    for file_path in files:\n",
    "        # Créer un fichier temporaire\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as temp_file:\n",
    "            local_input_path = temp_file.name\n",
    "\n",
    "        # Télécharger le fichier dans le répertoire temporaire\n",
    "        download_file(file_path, local_input_path)\n",
    "\n",
    "        # Valider et nettoyer les données\n",
    "        cleaned_file, error_file = validate_and_clean_data(local_input_path)\n",
    "        \n",
    "        if cleaned_file:\n",
    "            upload_file(cleaned_file, f\"clean/{os.path.basename(cleaned_file)}\")\n",
    "            load_to_bigquery(f\"clean/{os.path.basename(cleaned_file)}\")\n",
    "            move_to_done(f\"clean/{os.path.basename(cleaned_file)}\")\n",
    "        if error_file:\n",
    "            upload_file(error_file, f\"error/{os.path.basename(error_file)}\")\n",
    "\n",
    "        # Supprimer les fichiers temporaires\n",
    "        os.remove(local_input_path)\n",
    "        if cleaned_file and os.path.exists(cleaned_file):\n",
    "            os.remove(cleaned_file)\n",
    "        if error_file and os.path.exists(error_file):\n",
    "            os.remove(error_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Téléchargé : input/transactions.csv -> C:\\Users\\DIARRA\\AppData\\Local\\Temp\\tmpqmjsu9uu.csv\n",
      "INFO:root:Fichier nettoyé : C:\\Users\\DIARRA\\AppData\\Local\\Temp\\tmpqmjsu9uu_cleaned.csv\n",
      "INFO:root:Fichier d'erreurs : C:\\Users\\DIARRA\\AppData\\Local\\Temp\\tmpqmjsu9uu_errors.csv\n",
      "INFO:root:Uploadé : C:\\Users\\DIARRA\\AppData\\Local\\Temp\\tmpqmjsu9uu_cleaned.csv -> clean/tmpqmjsu9uu_cleaned.csv\n",
      "INFO:root:Données chargées dans BigQuery depuis : gs://m2dsia-sylla-mame-diarra-data/clean/tmpqmjsu9uu_cleaned.csv\n",
      "INFO:root:Fichier déplacé : clean/tmpqmjsu9uu_cleaned.csv -> done/tmpqmjsu9uu_doneed.csv\n",
      "INFO:root:Uploadé : C:\\Users\\DIARRA\\AppData\\Local\\Temp\\tmpqmjsu9uu_errors.csv -> error/tmpqmjsu9uu_errors.csv\n"
     ]
    }
   ],
   "source": [
    "process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fonction pour supprimer\n",
    "# def supprimer_blob(nom_fichier_cloud):\n",
    "#     blob = bucket.blob(nom_fichier_cloud)\n",
    "#     blob.delete()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
